{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# uncomment below if pyg not installed,\n",
    "# os.environ['TORCH'] = torch.__version__\n",
    "# print(torch.__version__)\n",
    "# !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "# !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "# !pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "import torch_geometric.nn as pyg_nn\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "from functools import partial\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_geometric.nn as pyg_nn\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from math import ceil\n",
    "\n",
    "from torch_geometric.data import DenseDataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a dataset\n",
    "\n",
    "We use the `Enymes` Dataset containing molecule graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: ENZYMES(600):\n",
      "======================\n",
      "Number of graphs: 600\n",
      "Number of features: 3\n",
      "Number of classes: 6\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import TUDataset\n",
    "\n",
    "dataset = TUDataset(name='ENZYMES', root='data/TUDataset')\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('======================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample graph: Data(edge_index=[2, 168], x=[37, 3], y=[1])\n",
      "==============================================================\n",
      "Number of avg. nodes: 32.63\n",
      "Number of avg. edges: 124.27\n"
     ]
    }
   ],
   "source": [
    "data = dataset[0]  # Get the first (and only) graph object.\n",
    "\n",
    "print(f\"Sample graph: {data}\")\n",
    "print('==============================================================')\n",
    "\n",
    "# Gather some statistics about the graph.\n",
    "print(f'Number of avg. nodes: {np.mean([data.num_nodes for data in dataset]):.2f}')\n",
    "print(f'Number of avg. edges: {np.mean([data.num_edges for data in dataset]):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-batching for graphs\n",
    "\n",
    "We make use of different dataloaders implemented in PyTorch Geometric: https://pytorch-geometric.readthedocs.io/en/latest/modules/loader.html#torch_geometric.loader.DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda3\\envs\\adlg\\lib\\site-packages\\torch_geometric\\deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "# Create a PyTorch Geometric DataLoader object for easy graph mini-batching.\n",
    "BATCH_SIZE = 16\n",
    "graph_dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch from the dataloader\n",
    "sample_batch = next(iter(graph_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's investigate the batch object we have received"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataBatch(edge_index=[2, 1948], x=[518, 3], y=[16], batch=[518], ptr=[17])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It contains a single `edge_index`, a single node feature matrix `x`,\n",
    "a single target label matrix `y`, and a batch indicator matrix `batch`.\n",
    "\n",
    "The dataloader merged all graphs into a single set of disjoint graphs.\n",
    "Standard message passing operators can natively run on this representation,\n",
    "because no messages are passed between the disjoint set of graphs.\n",
    "\n",
    "This allows for efficient mini-batching and parallel processing\n",
    "of different graphs, without any memory overhead of e.g. additional padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output size: torch.Size([518, 16])\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "# Create a GraphSAGE model\n",
    "conv = SAGEConv(dataset.num_features, 16)\n",
    "\n",
    "# Run the convolution operator\n",
    "out = conv(sample_batch.x, sample_batch.edge_index)\n",
    "\n",
    "# Check the size of the output\n",
    "print(f'Output size: {out.size()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have node embeddings for all the graphs, but what if we want to\n",
    "aggregate them into individual representations for each graph?\n",
    "\n",
    "We need to make use of the `batch` indicator attribute!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 518, batch indicator matrix: torch.Size([518])\n",
      "Batch size: 16, unique batch indicator values: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of nodes: {sample_batch.num_nodes}, batch indicator matrix: {sample_batch.batch.shape}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}, unique batch indicator values: {sample_batch.batch.unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph embedding shape: torch.Size([16, 16])\n"
     ]
    }
   ],
   "source": [
    "# this is one of the helper libraries recommended to install along pytorch geometric\n",
    "import torch_scatter\n",
    "\n",
    "# The `scatter` function supports a set of aggregations: https://pytorch-scatter.readthedocs.io/en/latest/functions/scatter.html\n",
    "graph_embeddings  = torch_scatter.scatter(out, sample_batch.batch, dim=0, reduce=\"mean\")\n",
    "print(f\"Graph embedding shape: {graph_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same but in an even simpler manner is also supported by PyG out-of-the box now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn.pool import global_mean_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph embedding shape: torch.Size([16, 16])\n"
     ]
    }
   ],
   "source": [
    "graph_embeddings = global_mean_pool(out, sample_batch.batch)\n",
    "print(f\"Graph embedding shape: {graph_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 Baseline GNN\n",
    "\n",
    "We refer to the paper Design Space of GNN (Jiaxuan You et al.), where a fundamental model contains the following blocks:\n",
    "\n",
    "1. Pre-processing MLP Layers\n",
    "2. Message Passing Layers (GNN + BN + Activation + Dropout)\n",
    "3. (*) Skip-connection Layers / Residual Blocks\n",
    "4. Pooling Layer(s). \n",
    "5. Post-processing MLP Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class basicGNN(nn.Module):\n",
    "    def __init__(self, depth, method, dropout, pool, input_dim, output_dim, embed_dim, connection):\n",
    "        super(basicGNN, self).__init__()\n",
    "        self.depth = depth\n",
    "        self.method = method\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dropout = dropout\n",
    "        self.connection = connection\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.preprocess_mlp = nn.ModuleList()\n",
    "        self.gnn_mlp = nn.ModuleList()\n",
    "        self.postprocess_mlp = nn.ModuleList()\n",
    "        self.pool = pool\n",
    "        \n",
    "        for i in range(2):\n",
    "            # Here the preprocess layers we refer to the paper: \n",
    "            # Design Space for Graph Neural Networks, Jiaxuan You et al. NeurIPS 2021\n",
    "            if i == 0:\n",
    "                self.preprocess_mlp.append(nn.Sequential(\n",
    "                        nn.Linear(self.input_dim, self.embed_dim),\n",
    "                        nn.BatchNorm1d(self.embed_dim),\n",
    "                        nn.PReLU(),\n",
    "                )) \n",
    "            else:\n",
    "                self.preprocess_mlp.append(nn.Sequential(\n",
    "                        nn.Linear(self.embed_dim, self.embed_dim),\n",
    "                        nn.BatchNorm1d(self.embed_dim),\n",
    "                        nn.PReLU(),\n",
    "                )) \n",
    "\n",
    "        for i in range(3):\n",
    "            # Here the preprocess layers we refer to the paper: \n",
    "            # Design Space for Graph Neural Networks, Jiaxuan You et al. NeurIPS 2021\n",
    "            if i != 2:\n",
    "                if i == 1:\n",
    "                    self.postprocess_mlp.append(nn.Sequential(\n",
    "                        nn.Linear(self.embed_dim, self.embed_dim),\n",
    "                        nn.BatchNorm1d(self.embed_dim),\n",
    "                        nn.PReLU(),\n",
    "                ))\n",
    "                else:\n",
    "                    if self.connection == \"residual\":\n",
    "                        self.postprocess_mlp.append(nn.Sequential(\n",
    "                            nn.Linear(self.embed_dim , self.embed_dim),\n",
    "                            nn.BatchNorm1d(self.embed_dim),\n",
    "                            nn.PReLU(),\n",
    "                    ))\n",
    "                    else:\n",
    "                        self.postprocess_mlp.append(nn.Sequential(\n",
    "                            nn.Linear(self.embed_dim * self.depth , self.embed_dim),\n",
    "                            nn.BatchNorm1d(self.embed_dim),\n",
    "                            nn.PReLU(),\n",
    "                        ))\n",
    "            else:\n",
    "                self.postprocess_mlp.append(nn.Sequential(\n",
    "                            nn.Linear(self.embed_dim, self.output_dim),\n",
    "                    ))\n",
    "        for i in range(self.depth):\n",
    "            # Here the MLP layers we also refer to the paper: \n",
    "            # Design Space for Graph Neural Networks, Jiaxuan You et al. NeurIPS 2021\n",
    "            # We apply ACT[DROPOUT[BN[Linear]]] in order\n",
    "            # Particularly, we use the result from the paper that dropout layer should be removed\n",
    "            self.gnn_mlp.append(nn.Sequential(\n",
    "                        nn.Linear(self.embed_dim, self.embed_dim),\n",
    "                        nn.BatchNorm1d(self.embed_dim),\n",
    "                        nn.Dropout(self.dropout),\n",
    "                        nn.PReLU(),\n",
    "                        nn.Linear(self.embed_dim, self.embed_dim),\n",
    "                        nn.BatchNorm1d(self.embed_dim),\n",
    "                        nn.Dropout(self.dropout),\n",
    "                        nn.PReLU(),\n",
    "                )) \n",
    "\n",
    "        for i in range(self.depth):\n",
    "            # Here we only provide only four mainstream graph encoders for baseline tests:\n",
    "            # GCN, GraphSAGE, GAT and GIN\n",
    "            if method == \"GAT\":\n",
    "                self.convs.append(pyg_nn.GATConv(self.embed_dim, self.embed_dim))\n",
    "            elif method == \"GIN\":\n",
    "                self.convs.append(pyg_nn.GINConv(self.gnn_mlp[i]))\n",
    "            elif method == \"GraphSAGE\":\n",
    "                self.convs.append(pyg_nn.SAGEConv(self.embed_dim, self.embed_dim, normalize=True)) \n",
    "            elif method == \"GCN\":\n",
    "                self.convs.append(pyg_nn.GCNConv(self.embed_dim, self.embed_dim))      \n",
    "\n",
    "    def forward(self, x, edge_index, batch, mask = None):\n",
    "        # 1. preprocess \n",
    "        self.save_results = []\n",
    "        h = self.preprocess_mlp[0](x)\n",
    "        h = self.preprocess_mlp[1](h)\n",
    "        # 2. residual / skip-connect\n",
    "        for i in range(self.depth):\n",
    "            if self.method == \"GIN\":\n",
    "                if self.connection == \"residual\":\n",
    "                    h = h + self.convs[i](h, edge_index)              \n",
    "                else:\n",
    "                    h = self.convs[i](h, edge_index)\n",
    "                    self.save_results.append(h)\n",
    "            else:\n",
    "                if self.connection == \"residual\":\n",
    "                    h = h + self.convs[i](self.gnn_mlp[i](h), edge_index)\n",
    "\n",
    "                else:\n",
    "                    h = self.convs[i](self.gnn_mlp[i](h), edge_index)  \n",
    "                    self.save_results.append(h)    \n",
    "         \n",
    "   \n",
    "        if self.connection == \"skip\":\n",
    "            h= torch.cat(self.save_results, dim=1)\n",
    "        # 3. pooling for graph classification\n",
    "\n",
    "        if self.pool == \"mean\":\n",
    "            h = pyg_nn.global_mean_pool(h, batch) \n",
    "        elif self.pool == \"max\":\n",
    "            h = pyg_nn.global_max_pool(h, batch)\n",
    "\n",
    "        # 4. postprocess\n",
    "        h = self.postprocess_mlp[0](h)\n",
    "        h = self.postprocess_mlp[1](h) # output\n",
    "        h = self.postprocess_mlp[2](h) # output\n",
    "        out = F.log_softmax(h, dim =1)\n",
    "\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(dataset, seed):\n",
    "    torch.manual_seed(seed)\n",
    "    return dataset.shuffle()\n",
    "\n",
    "def train_test_val_split(num_test, batch_size, dataset):\n",
    "    test_dataset = dataset[:num_test]\n",
    "    val_dataset = dataset[num_test:2 * num_test]\n",
    "    train_dataset = dataset[2 * num_test:]\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "    return test_loader, val_loader, train_loader\n",
    "\n",
    "def train(epoch, model, train_loader, device, optimizer):\n",
    "    model.train()\n",
    "    loss_all = 0\n",
    "    for data in train_loader:\n",
    "        model.train()\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.x, data.edge_index, data.batch)\n",
    "        loss = F.nll_loss(output, data.y.view(-1))\n",
    "        loss.backward()\n",
    "        loss_all += loss.item() * len(data.y)\n",
    "        optimizer.step()\n",
    "    return loss_all / len(train_loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader, model, device, ):\n",
    "    correct = 0\n",
    "    model.eval()\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        pred = model(data.x, data.edge_index, data.batch).max(dim=1)[1]\n",
    "        correct += pred.eq(data.y).sum().item()\n",
    "    return correct / len(loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for seed i: 0, model GraphSAGE has the best test accuracy: 0.58\n",
      "for seed i: 0, model GraphSAGE has the best test accuracy: 0.61\n",
      "for seed i: 0, model GraphSAGE has the best test accuracy: 0.5\n",
      "for seed i: 0, model GraphSAGE has the best test accuracy: 0.56\n",
      "for seed i: 0, model GraphSAGE has the best test accuracy: 0.53\n",
      "for seed i: 42, model GraphSAGE has the best test accuracy: 0.53\n",
      "for seed i: 42, model GraphSAGE has the best test accuracy: 0.5\n",
      "for seed i: 42, model GraphSAGE has the best test accuracy: 0.57\n",
      "for seed i: 42, model GraphSAGE has the best test accuracy: 0.53\n",
      "for seed i: 42, model GraphSAGE has the best test accuracy: 0.53\n",
      "for seed i: 418004, model GraphSAGE has the best test accuracy: 0.56\n",
      "for seed i: 418004, model GraphSAGE has the best test accuracy: 0.58\n",
      "for seed i: 418004, model GraphSAGE has the best test accuracy: 0.53\n",
      "for seed i: 418004, model GraphSAGE has the best test accuracy: 0.55\n",
      "for seed i: 418004, model GraphSAGE has the best test accuracy: 0.58\n",
      "average accuracy 0.5493333333333333 with 0.030433899228035525 for method GraphSAGE, with seeds [0, 0, 0, 0, 0, 42, 42, 42, 42, 42, 418004, 418004, 418004, 418004, 418004]\n"
     ]
    }
   ],
   "source": [
    "seed = [0]*5 + [42]*5 + [418004]*5\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "# ------------ hyperparameters --------------- #\n",
    "modelname = \"GraphSAGE\"\n",
    "depth = 2\n",
    "dropout = 0.15\n",
    "aggr = \"mean\"\n",
    "embed_dim = 128\n",
    "connection = \"skip\"\n",
    "lr = 0.002\n",
    "weight_decay = 5e-4\n",
    "batch_size = 64\n",
    "epochs = 801\n",
    "num_test = 100 # 100 for test\n",
    "test_acc_ = []\n",
    "# ------------- begin training session  -----------------#\n",
    "# Shuffle dataset three times and train each dataset 5 times. then take average of them (15)\n",
    "for i in range(15):\n",
    "    dataset = shuffle(TUDataset(name='ENZYMES', root='data/TUDataset'), seed[i])\n",
    "    torch.manual_seed(seed=seed[i])\n",
    "    model = basicGNN(depth, modelname, dropout, aggr, dataset.num_features, \n",
    "                      dataset.num_classes, embed_dim, connection).to(device)\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr = lr, weight_decay=weight_decay)\n",
    "    test_loader, val_loader, train_loader = train_test_val_split(num_test, batch_size, dataset)\n",
    "    best_val_acc = test_acc = 0\n",
    "    times = []\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train(epoch, model, train_loader, device, optimizer)\n",
    "        val_acc = test(val_loader, model, device)\n",
    "        if val_acc > best_val_acc:\n",
    "            test_acc = test(test_loader, model, device)\n",
    "            best_val_acc = val_acc\n",
    "        # if epoch % 100 == 0:\n",
    "        #     print(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.4f}, '\n",
    "        #         f'Best Val Acc: {best_val_acc:.4f}, Best Test Acc: {test_acc:.4f}')\n",
    "    test_acc_.append(test_acc)\n",
    "    print(\"for seed i: {}, model {} has the best test accuracy: {}\".format(seed[i], modelname, test_acc))\n",
    "\n",
    "print(\"average accuracy {} with {} for method {}, with seeds {}\".format(np.mean(test_acc_), np.std(test_acc_), modelname, seed))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 Improvement on pooling\n",
    "\n",
    "Diffpool (Rex et al.) 2018 KDD states that a connection of hierarchical pooling layers will lead to better performance with GraphSAGE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Graph Network with Hierarchical DiffPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels,\n",
    "                 normalize=False, lin=True):\n",
    "        super().__init__()\n",
    "        self.conv1 = pyg_nn.DenseSAGEConv(in_channels, hidden_channels, normalize)\n",
    "        self.conv2 = pyg_nn.DenseSAGEConv(hidden_channels, hidden_channels, normalize)\n",
    "        self.conv3 = pyg_nn.DenseSAGEConv(hidden_channels, out_channels, normalize)\n",
    "        self.lin = torch.nn.Linear(2 * hidden_channels + out_channels,\n",
    "                                       out_channels) if lin is True else None\n",
    "   \n",
    "    def forward(self, x, adj, mask=None):\n",
    "        x0 = x\n",
    "        x1 =(self.conv1(x0, adj, mask).relu())\n",
    "        x2 =(self.conv2(x1, adj, mask).relu())\n",
    "        x3 =(self.conv3(x2, adj, mask).relu())\n",
    "        x = torch.cat([x1, x2, x3], dim=-1)\n",
    "        if self.lin is not None:\n",
    "            x = self.lin(x).relu()\n",
    "        return x\n",
    "\n",
    "class DiffPoolGNN(torch.nn.Module):\n",
    "    def __init__(self, assign_ratio, embed_dim, output_dim, max_nodes):\n",
    "        super().__init__()\n",
    "        num_nodes = ceil(assign_ratio * max_nodes)\n",
    "        self.gnn1_pool = GNN(dataset.num_features, embed_dim, num_nodes)\n",
    "        self.gnn1_embed = GNN(dataset.num_features, embed_dim, embed_dim, lin=False)\n",
    "        num_nodes = ceil(assign_ratio * num_nodes)\n",
    "        self.gnn2_pool = GNN(3 * embed_dim, embed_dim, num_nodes)\n",
    "        self.gnn2_embed = GNN(3 * embed_dim, embed_dim, embed_dim, lin=False)\n",
    "        self.gnn3_pool = GNN(3 * embed_dim, embed_dim, num_nodes) \n",
    "        self.gnn3_embed = GNN(3 * embed_dim, embed_dim, embed_dim, lin=False)\n",
    "        self.gnn4_embed = GNN(3 * embed_dim, embed_dim, embed_dim, lin=False)\n",
    "        self.lin1 = torch.nn.Linear(3 * embed_dim, embed_dim)\n",
    "        self.lin2 = torch.nn.Linear(embed_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, adj, mask=None):\n",
    "        s = self.gnn1_pool(x, adj, mask)\n",
    "        x = self.gnn1_embed(x, adj, mask)\n",
    "        x, adj, l1, e1 = pyg_nn.dense_diff_pool(x, adj, s, mask)\n",
    "        s = self.gnn2_pool(x, adj)\n",
    "        x = self.gnn2_embed(x, adj)\n",
    "        x, adj, l2, e2 = pyg_nn.dense_diff_pool(x, adj, s)\n",
    "        # s = self.gnn3_pool(x, adj)\n",
    "        x = self.gnn3_embed(x, adj)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.lin1(x).relu()\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1), l1 + l2, e1 + e2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_val_split(num_test, batch_size, dataset):\n",
    "    test_dataset = dataset[:num_test]\n",
    "    val_dataset = dataset[num_test: int(2.5 * num_test)]\n",
    "    train_dataset = dataset[int(2.5 * num_test):]\n",
    "    test_loader = DenseDataLoader(test_dataset, batch_size=batch_size)\n",
    "    val_loader = DenseDataLoader(val_dataset, batch_size=batch_size)\n",
    "    train_loader = DenseDataLoader(train_dataset, batch_size=batch_size)\n",
    "    return test_loader, val_loader, train_loader\n",
    "\n",
    "def train(epoch, model, train_loader, device, optimizer):\n",
    "    model.train()\n",
    "    loss_all = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        output, _, _ = model(data.x, data.adj, data.mask)\n",
    "        loss = F.nll_loss(output, data.y.view(-1))\n",
    "        loss.backward()\n",
    "        loss_all += data.y.size(0) * float(loss)\n",
    "        optimizer.step()\n",
    "    return loss_all / len(train_loader.dataset)\n",
    "    # return loss_all / len(train_dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader, model, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        pred = model(data.x, data.adj, data.mask)[0].max(dim=1)[1]\n",
    "        correct += int(pred.eq(data.y.view(-1)).sum())\n",
    "    return correct / len(loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Epoch: 050, Train Loss: 1.0716, Best Val Acc: 0.4600, Best Test Acc: 0.4600\n",
      "Epoch: 100, Train Loss: 0.5902, Best Val Acc: 0.5200, Best Test Acc: 0.5800\n",
      "Epoch: 150, Train Loss: 0.3310, Best Val Acc: 0.5467, Best Test Acc: 0.6200\n",
      "Epoch: 200, Train Loss: 0.0226, Best Val Acc: 0.5467, Best Test Acc: 0.6200\n",
      "Epoch: 250, Train Loss: 0.1654, Best Val Acc: 0.5467, Best Test Acc: 0.6200\n",
      "Epoch: 300, Train Loss: 0.0240, Best Val Acc: 0.5467, Best Test Acc: 0.6200\n",
      "Epoch: 350, Train Loss: 0.0255, Best Val Acc: 0.5467, Best Test Acc: 0.6200\n",
      "Epoch: 400, Train Loss: 0.0264, Best Val Acc: 0.5467, Best Test Acc: 0.6200\n",
      "for seed i: 1, model has the best test accuracy: 0.62\n",
      "Epoch: 050, Train Loss: 0.9263, Best Val Acc: 0.4933, Best Test Acc: 0.3400\n",
      "Epoch: 100, Train Loss: 0.3385, Best Val Acc: 0.5133, Best Test Acc: 0.4500\n",
      "Epoch: 150, Train Loss: 0.0926, Best Val Acc: 0.5200, Best Test Acc: 0.6000\n",
      "Epoch: 200, Train Loss: 0.1025, Best Val Acc: 0.5200, Best Test Acc: 0.6000\n",
      "Epoch: 250, Train Loss: 0.0531, Best Val Acc: 0.5200, Best Test Acc: 0.6000\n",
      "Epoch: 300, Train Loss: 0.0170, Best Val Acc: 0.5200, Best Test Acc: 0.6000\n",
      "Epoch: 350, Train Loss: 0.0885, Best Val Acc: 0.5200, Best Test Acc: 0.6000\n",
      "Epoch: 400, Train Loss: 0.0132, Best Val Acc: 0.5200, Best Test Acc: 0.6000\n",
      "for seed i: 67, model has the best test accuracy: 0.6\n",
      "Epoch: 050, Train Loss: 1.0219, Best Val Acc: 0.4533, Best Test Acc: 0.4800\n",
      "Epoch: 100, Train Loss: 0.6195, Best Val Acc: 0.5200, Best Test Acc: 0.5300\n",
      "Epoch: 150, Train Loss: 0.1086, Best Val Acc: 0.5267, Best Test Acc: 0.5900\n",
      "Epoch: 200, Train Loss: 0.0820, Best Val Acc: 0.5533, Best Test Acc: 0.6100\n",
      "Epoch: 250, Train Loss: 0.0143, Best Val Acc: 0.5533, Best Test Acc: 0.6100\n",
      "Epoch: 300, Train Loss: 0.0475, Best Val Acc: 0.5533, Best Test Acc: 0.6100\n",
      "Epoch: 350, Train Loss: 0.1772, Best Val Acc: 0.5533, Best Test Acc: 0.6100\n",
      "Epoch: 400, Train Loss: 0.0120, Best Val Acc: 0.5533, Best Test Acc: 0.6100\n",
      "for seed i: 418004, model has the best test accuracy: 0.61\n",
      "average accuracy 0.61 with 0.008164965809277268 for diffpool, with seeds [1, 67, 418004]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from math import ceil\n",
    "\n",
    "max_nodes = 300\n",
    "dataset = TUDataset(\n",
    "    # path,\n",
    "    name='ENZYMES',\n",
    "    root='data/TUDataset',\n",
    "    transform=T.ToDense(max_nodes),\n",
    "    pre_filter=lambda data: data.num_nodes <= max_nodes,\n",
    ")\n",
    "\n",
    "num_test = 100\n",
    "assign_ratio = 0.1\n",
    "embed_dim = 64\n",
    "output_dim = dataset.num_classes\n",
    "batch_size = 25\n",
    "seed = [1, 67, 418004] # 1, 67, 418004 many other seeds also suitable such as 42 for V100 Colab\n",
    "lr = 0.001 \n",
    "epochs = 450\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "print(device)\n",
    "test_acc_= []\n",
    "\n",
    "for i in seed:\n",
    "    torch.manual_seed(i)\n",
    "    dataset_ = dataset.shuffle()\n",
    "    test_loader, val_loader, train_loader = train_test_val_split(num_test, batch_size, dataset_)\n",
    "    model = DiffPoolGNN(assign_ratio, embed_dim, output_dim, max_nodes).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_val_acc = test_acc = 0\n",
    "    times = []\n",
    "    for epoch in range(1, epochs):\n",
    "        train_loss = train(epoch, model, train_loader, device, optimizer)\n",
    "        val_acc = test(val_loader, model, device)\n",
    "        if val_acc > best_val_acc:\n",
    "            test_acc = test(test_loader, model, device)\n",
    "            best_val_acc = val_acc\n",
    "        if epoch % 50 == 0:\n",
    "            print(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.4f}, '\n",
    "            f'Best Val Acc: {best_val_acc:.4f}, Best Test Acc: {test_acc:.4f}')\n",
    "    test_acc_.append(test_acc)\n",
    "    print(\"for seed i: {}, model has the best test accuracy: {}\".format(i, test_acc))\n",
    "\n",
    "print(\"average accuracy {} with {} for diffpool, with seeds {}\".format(np.mean(test_acc_), np.std(test_acc_), seed))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.62, 0.6, 0.61]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adlg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
